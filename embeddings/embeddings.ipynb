{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\">Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are when we translate sentences, words, images, audio, and other forms of data into numbers usually in teh format of a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](embedding_intuition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a model like a neural network on text data like the \"hello world\" seen below, we need to have an input vector that only contains vectors.\n",
    "\n",
    "We can do this my creating a matrix so that every possible character (letters, symbols, numbers, and spaces) can have a unique indetifiable matrix that can represent it in the input of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can than be used to predict the next letter (output vector) using character pairs like the ones seen below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](letters_into_NN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network can have an intermediary step where we can create a matrix of almost any size. \n",
    "\n",
    "One of those sizes is, obviously, a matrix of size 2, which we can then use the numbers to visualize on 2D plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](intermediate_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start, these plots will look pretty unorganized. \n",
    "\n",
    "But with more training and epochs, we can start to see shifts in the coordinates where clusters start to form than diffirentiate between consonant, vowels, numbers and the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](plots_through_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a more difficult task but a much more rewarding task is to use words instead of letters. Since the number of letters were limited, the vectors were much simpler to work with and a size 2 matrix for the intermediary was sufficient. Unofortunatly, words are much more complicated, meaning the input and output vectors are much more complex, and the intermediary matrix needs to be as large as 300.\n",
    "\n",
    "Word embeddings can then be plot like the aforementioned letter clusters. This allows words to also be clustered based on categories such as animals or food.\n",
    "\n",
    "Training upfront can also help create these clusters which can then be used help predict if these words belong to a recipe and then other words that are close to it in the cluster can be associated with a recipe as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](words.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to always be aware of when doing word embeddings is that the neural network learns on itself and the words around the target word to understand the context. What it does not do is actually know the meaning of the target word. For example we adjectives can be a victim of this key difference because two adjectives that mean opposite things (fast/slow, hot/cold, big/small) can be used in the exact same spot in a sentence. Thus, these words can also appear close to each other in a cluster even if they are complete opposites. Overall it is important to understand the limitations of word embeddings so that we can better interepert the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models with these types of embeddings are extremely flexable depending on the task you are trying to accomplish.\n",
    "\n",
    "Instead of the output being a vector that predicts the next word, we can input whole sentances and have the model predict labels, such as if that sentence was negative, positive, or about food.\n",
    "\n",
    "We can even create a model that helps create a vector that helps with any of the output labels/tasks where the vector representation can be used for a classification algorithim. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](flexable_pos.png) ![image.png](flex_label.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaking on its flexability, we can take this further by, instead of text, we embed an image using a convolusional layer which can then allow you to transform it into a k-sized array.\n",
    "\n",
    "If we also had some text and transformed that into another k-sized array, we can then now compare an image and some text to see if they are similar because now they map to the same space.\n",
    "\n",
    "This is what is called multi-modal because we are comparing two different types of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](text_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimatly all these different tasks, especially the aforementioned multi-modal technique might require some babysitting. This is where we provide obvious training data and corrections to clear connections the model is making so that the output is better."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
